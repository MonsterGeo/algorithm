# 第一章

## 算法的定义

简单来说，算法是任何的良定义的计算过程，这个过程接收某个值的集合作为输入并产生某个值或值的集合作为输出。这样算法就是把输入转换为输出的计算步骤的一个序列。

翻译过来就是，给定well-defined的函数$f(x),x\in A$，$A$是集合，其中$f$是算法，把$x$变为$f(x)$作为输出。那么取遍集合$A$中的每个元素组成的集合就是一个序列$f(x_1),f(x_2),\cdots$

例如，我们需要把一个数列排成非递减序列，那么这个排序问题可以这样定义：

---

**输入：** n个数的一个序列$(a_1,a_2,\cdots,a_n)$
**输出：** 输出序列的一个排列$(a_1',a_2',\cdots,a'_n)$，满足$a_1' \leq a_2'\leq \cdots \leq a_n'$

---

例如，我们给定输入序列(31,41,59,26,41,58)，排序算法返回序列(26,31,41,41,58,59)。作为输出。这样的输入序列我们称为一个实例(instance)。一般来说，问题实例由计算该问题解所必须的输入(即满足问题陈述中的各种约束条件)组成。


另一方面，若对每个输入实例算法都以正确的输出停机，则我们说算法是正确的，并称正确的算法解决了给定的计算问题。不正确的算法对某些输入实例可能根本就不停机。也可能以不正确的回答停机。然而，不正确的算法只要其错误率可控有时候可能是有用的，例如大素数算法，已知目前没有精确的定理可以描述素数的分布，但我们可以通过控制其错误率来得到一个素数。但我们一般只关心正确的算法。

算法可以用英文说明，可以用计算机程序说明，也可以说明成硬件设计，唯一的要求是这个说明必须精确描述所遵循的计算过程

## 作为一种技术的算法

我们假设计算机是无限快的并且计算机的存储器将是免费的，那么算法的效率将只取决于算法的复杂性。然而，现实世界中的计算机并没有无限快的处理器，并且计算机的存储器是有限的。因此，算法的效率不仅取决于算法的复杂性，还取决于计算机的硬件。因此，算法的效率不仅取决于算法的复杂性，还取决于计算机的硬件。

### 效率

为求解相同问题设计的不同算法再效率方面常常具有显著的差别，这些差别可能由于硬件和软件造成的差别咬重要得多

例如，对于插入排序，为了排序n个项，该算法所花费的时间大概再$c_1n^2$，其中$c_1$是一个不依赖于n的常数。也就是说，该算法运行所花费时间和$n^2$成正比。第二个是归并排序，为了排序n个项，所耗费的时间大概是$c_2n \lg n$，其中$\lg n = \log_2 n$切$c_2$是另一个常数。我们将看到，对于常数$c_1 < c_2$，它决定算法的运算时间的重要性可能远没有输入规模n的重要性大。

如果你学过高等数学，那么这是很容易看出来的，只需要计算极限

$$
\lim_{n\to\infty}\frac{c_1n^2}{c_2n\lg n}=\lim_{n\to\infty}\frac{c_1}{c_2}\frac{n}{\lg n}=\infty
$$

所以，对于足够大的n，$c_1n^2$将远远大于$c_2n\lg n$。因此，对于足够大的n，归并排序将比插入排序更快。

如果你没学过，那也没有关系例如，档$n = 1000$时，$\lg n$大概是10，而$n$是100万的时候，$\lg n$大概是20。虽然在输入规模小的时候，插入排序可能比归并排序更快，但对于足够大的输入规模，$\lg n$的补偿将会弥补这一点，不管$c_1$比$c_2$小多少，总会存在一个交叉点，在超出这个交叉点之后归并排序总是比插入排序更快。


